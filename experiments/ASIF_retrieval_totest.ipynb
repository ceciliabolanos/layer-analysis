{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASIF - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Type, Union\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def relative_represent(y: torch.Tensor, basis: torch.Tensor, non_zeros: int = 800, max_gpu_mem_gb: int = 8) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute the sparse decomposition of a tensor y with respect to a basis, \n",
    "    considering the available GPU memory.\n",
    "    \n",
    "    Args:\n",
    "        y (torch.Tensor): Vectors to represent.\n",
    "        basis (torch.Tensor): Basis to represent with respect to.\n",
    "        non_zeros (int): Nonzero entries in the relative representation.\n",
    "        max_gpu_mem_gb (int): Maximum GPU memory allowed to use in gigabytes.\n",
    "        \n",
    "    Returns:\n",
    "        indices (torch.Tensor): Indices of the nonzero entries in each relative representation of y.\n",
    "        values (torch.Tensor): Corresponding coefficients of the entries.\n",
    "    \"\"\"\n",
    "    values, indices = torch.zeros((y.shape[0], non_zeros)), torch.zeros((y.shape[0], non_zeros), dtype=torch.long)\n",
    "\n",
    "    free_gpu_mem = max_gpu_mem_gb * 1024 ** 3\n",
    "    max_floats_in_mem = free_gpu_mem / 4\n",
    "    max_chunk_y = max_floats_in_mem / basis.shape[0]\n",
    "    n_chunks = int(y.shape[0] / max_chunk_y) + 1  \n",
    "    chunk_y = int(y.shape[0] / n_chunks) + n_chunks\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for c in range(n_chunks):\n",
    "            in_prods = torch.einsum('ik, jk -> ij', y[c * chunk_y : (c + 1) * chunk_y], basis)\n",
    "            values[c * chunk_y : (c + 1) * chunk_y], indices[c * chunk_y : (c + 1) * chunk_y] = torch.topk(in_prods, non_zeros, dim=1)\n",
    "            del in_prods\n",
    "\n",
    "    return indices.to('cpu'), values.to('cpu')\n",
    "\n",
    "\n",
    "def sparsify(i: torch.Tensor, v: torch.Tensor, size: torch.Size) -> torch.sparse.FloatTensor:\n",
    "    \"\"\"\n",
    "    Organize indices and values of n vectors into a single sparse tensor.\n",
    "\n",
    "    Args:\n",
    "        i (torch.Tensor): indices of non-zero elements of every vector. Shape: (n_vectors, nonzero elements)\n",
    "        v (torch.Tensor): values of non-zero elements of every vector. Shape: (n_vectors, nonzero elements)\n",
    "        size (torch.Size): shape of the output tensor\n",
    "\n",
    "    Returns:\n",
    "        torch.sparse.FloatTensor: sparse tensor of shape \"size\" (n_vectors, zero + nonzero elements)\n",
    "    \"\"\"\n",
    "    flat_dim = len(i.flatten())\n",
    "    coo_first_row_idxs = torch.div(torch.arange(flat_dim), i.shape[1], rounding_mode='floor')\n",
    "    stacked_idxs = torch.cat((coo_first_row_idxs.unsqueeze(0), i.flatten().unsqueeze(0)), 0)\n",
    "    return torch.sparse_coo_tensor(stacked_idxs, v.flatten(), size)\n",
    "\n",
    "\n",
    "def normalize_sparse(tensor: torch.sparse.FloatTensor, nnz_per_row: int) -> torch.sparse.FloatTensor:\n",
    "    \"\"\"\n",
    "    Normalize a sparse tensor by row.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.sparse.FloatTensor): The sparse tensor to normalize.\n",
    "        nnz_per_row (int): The number of non-zero elements per row.\n",
    "\n",
    "    Returns:\n",
    "        torch.sparse.FloatTensor: The normalized sparse tensor.\n",
    "    \"\"\"\n",
    "    norms = torch.sparse.sum(tensor * tensor, dim=1).to_dense()\n",
    "    v = tensor._values().clone().detach().reshape(-1, nnz_per_row).t()\n",
    "    v /= torch.sqrt(norms)\n",
    "    return torch.sparse_coo_tensor(tensor._indices(), v.t().flatten(), tensor.shape)\n",
    "\n",
    "\n",
    "def zero_shot_classification(zimgs: torch.Tensor, ztxts: torch.Tensor, aimgs: torch.Tensor, atxts: torch.Tensor, non_zeros: int, range_anch: range, val_exps: list, dic_size: int = 100_000, max_gpu_mem_gb: float = 8.) -> (list, dict, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Computes the zero-shot classification accuracy using relative representations\n",
    "    over sets of anchors of different sizes and raising the similarities to the given exponents.\n",
    "    \n",
    "    Args:\n",
    "        zimgs (torch.Tensor): absolute embeddings of the images\n",
    "        ztxts (torch.Tensor): absolute embeddings of the texts\n",
    "        aimgs (torch.Tensor): absolute embeddings of the anchor images\n",
    "        atxts (torch.Tensor): absolute embeddings of the anchor texts\n",
    "        test_labels (list): ground truth labels of the images\n",
    "        non_zeros (int): nonzero entries in the relative representation (k in paper)\n",
    "        range_anch (range): range of sizes of the anchor's sets to use (overshoot is ok)\n",
    "        dic_size (int): size of the chunk of aimgs to load in memory to fit all intermediate variables in RAM\n",
    "        val_exps (list): similarity exponents to test\n",
    "\n",
    "    Returns:\n",
    "        n_anchors (list): list of sizes of the anchor's sets (with overshooting fixed)\n",
    "        scores (dict): dictionary of scores for each tested similarity exponent\n",
    "        sims (torch.Tensor): similarity matrix between images and texts\n",
    "    \"\"\"\n",
    "    n_anchors = []\n",
    "    #scores = {ve: [] for ve in val_exps}\n",
    "    #n_templates = max(int(ztxts.shape[0] / (max(test_labels) - min(test_labels) + 1)), 1)\n",
    "\n",
    "    for i in tqdm(range_anch, position=0, leave=True):\n",
    "        sims = torch.zeros((len(zimgs), len(ztxts)))\n",
    "        idxs_imgs = torch.zeros(((len(zimgs), non_zeros * 2)), dtype=torch.long)\n",
    "        idxs_txts = torch.zeros(((len(ztxts), non_zeros * 2)), dtype=torch.long)\n",
    "        vals_imgs = torch.zeros(((len(zimgs), non_zeros * 2)))\n",
    "        vals_txts = torch.zeros(((len(ztxts), non_zeros * 2)))\n",
    "\n",
    "        for d in range(min(len(aimgs), i) // (dic_size + 1) + 1):\n",
    "            idxs, values = relative_represent(zimgs, aimgs[d * dic_size : min(i, (d + 1) * dic_size)], non_zeros=non_zeros, max_gpu_mem_gb=max_gpu_mem_gb)\n",
    "            idxs_imgs[:, :non_zeros] = idxs + d * dic_size\n",
    "            vals_imgs[:, :non_zeros] = values\n",
    "            idxs, values = relative_represent(ztxts, atxts[d * dic_size : min(i, (d + 1) * dic_size)], non_zeros=non_zeros, max_gpu_mem_gb=max_gpu_mem_gb)\n",
    "            idxs_txts[:, :non_zeros] = idxs + d * dic_size\n",
    "            vals_txts[:, :non_zeros] = values\n",
    "\n",
    "            top_valsi, indices = torch.topk(vals_imgs, non_zeros, dim=1)\n",
    "            top_idxsi = torch.gather(idxs_imgs, 1, indices)\n",
    "            top_valst, indices = torch.topk(vals_txts, non_zeros, dim=1)\n",
    "            top_idxst = torch.gather(idxs_txts, 1, indices)\n",
    "\n",
    "            idxs_imgs[:, non_zeros:] = top_idxsi\n",
    "            vals_imgs[:, non_zeros:] = top_valsi\n",
    "            idxs_txts[:, non_zeros:] = top_idxst\n",
    "            vals_txts[:, non_zeros:] = top_valst\n",
    "\n",
    "        for val_exp in val_exps:\n",
    "            ztxts_t = sparsify(top_idxst, top_valst ** val_exp, (len(ztxts), min(len(aimgs), i))).to(zimgs.device)\n",
    "            ztxts_t = normalize_sparse(ztxts_t, non_zeros)\n",
    "\n",
    "            if i < max_gpu_mem_gb * 1024 ** 3 / 4 / zimgs.shape[0]:  # einsum until it fits in GPU memory\n",
    "                zimgs_t = sparsify(top_idxsi, top_valsi ** val_exp, (len(zimgs), min(len(aimgs), i))).to(zimgs.device)\n",
    "                sims = torch.einsum('ij, kj -> ik', zimgs_t.to_dense(), ztxts_t.to_dense()).to('cpu')\n",
    "            else:\n",
    "                n_chunks = 6\n",
    "                zs = zimgs.shape[0]\n",
    "                chunks = [c * (zs // n_chunks) for c in range(n_chunks)] + [zs]\n",
    "                for ci in range(n_chunks):\n",
    "                    zimgs_t = sparsify(top_idxsi[chunks[ci]:chunks[ci+1]], top_valsi[chunks[ci]:chunks[ci+1]] ** val_exp, (chunks[ci+1] - chunks[ci], min(len(aimgs), i))).to(zimgs.device)\n",
    "                    sims[chunks[ci]:chunks[ci+1]] = torch.sparse.mm(zimgs_t, ztxts_t.t()).to('cpu').to_dense()\n",
    "            #score = float((torch.div(sims.argmax(axis=1),  n_templates, rounding_mode='floor') == torch.tensor(test_labels)).sum() / len(zimgs))\n",
    "            #scores[val_exp].append(score)\n",
    "        n_anchors.append(min(len(aimgs), i))\n",
    "    return n_anchors, sims\n",
    "\n",
    "\n",
    "def rand_mul_indices(indices_list: List[int], n_templates: int) -> torch.Tensor:\n",
    "    \"\"\"Returns a tensor containing randomly generated indices, based on the input indices_list and n_templates.\n",
    "\n",
    "    Args:\n",
    "        indices_list (List[int]): A list of integers representing the starting indices.\n",
    "        n_templates (int): An integer representing the number of templates.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing randomly generated indices.\n",
    "    \"\"\"\n",
    "    x = torch.randint(low=0, high=len(n_templates), size=(len(indices_list),))\n",
    "    return torch.tensor(indices_list) * len(n_templates) + x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "with open('../../experiments/layers/embeddings_layer6_wav2vec2.json', 'r') as f:\n",
    "    audio = np.array(json.load(f))\n",
    "\n",
    "with open('../../experiments/layers/embeddings_layer3_bert-base-uncased.json', 'r') as f:\n",
    "    nlp = np.array(json.load(f))\n",
    "\n",
    "with open('../words_in_order1.json', 'r') as f:\n",
    "    keys = json.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = [value for values_list in keys.values() for value in values_list]\n",
    "n = audio.shape[0]\n",
    "np.random.seed(2211) \n",
    "rows_to_delete = np.random.choice(n, int(n*0.1), replace=False) # Eliminamos el 10% de las filas para retrieval\n",
    "deleted_rows = audio[rows_to_delete]  # Guardamos los indices de las filas eliminadas\n",
    "audio_new = np.delete(audio, rows_to_delete, axis=0)\n",
    "nlp_new = np.delete(nlp, rows_to_delete, axis=0)\n",
    "keys_new = np.delete(all_values, rows_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_test = [all_values[i] for i in rows_to_delete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_new = torch.from_numpy(audio_new)\n",
    "nlp_new = torch.from_numpy(nlp_new)\n",
    "to_predict = torch.from_numpy(deleted_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:34<00:00, 34.18s/it]\n"
     ]
    }
   ],
   "source": [
    "total_anchors = len(audio_new)  # Assuming aimgs and atxts are the same length\n",
    "range_anch = range(total_anchors, total_anchors + 1)\n",
    "\n",
    "n_anchors, sims = zero_shot_classification(to_predict, nlp_new, audio_new, nlp_new, non_zeros=4221, range_anch = range_anch, val_exps=[1], dic_size = 100_000, max_gpu_mem_gb = 8.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a number i from 0 to 4600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word to test if the retrieval works\n",
    "keys_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the result\n",
    "values, indices = torch.topk(sims[i], k=6)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the first or another retrieval\n",
    "keys_new[indices[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
