{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze word and phones counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_phones(alignments, words_counts, phones_counts):\n",
    "    \"\"\"\n",
    "    Count the number of words and phones\n",
    "    \"\"\"\n",
    "    for key in alignments:\n",
    "        for xmin, xmax, text in alignments[key]:\n",
    "            if key == 'phones':\n",
    "                if text not in phones_counts:\n",
    "                   phones_counts[text] = 0\n",
    "                phones_counts[text] += 1\n",
    "            if key == 'words':\n",
    "                if text not in words_counts:\n",
    "                   words_counts[text] = 0\n",
    "                words_counts[text] += 1\n",
    "    return words_counts, phones_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from alignments.utils_alignments import parse_textgrid\n",
    "\n",
    "alignments_dir = '../datasets/alignments'\n",
    "\n",
    "words_counts = {}\n",
    "phones_counts = {}\n",
    "\n",
    "for root, dirs, files in os.walk(alignments_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".TextGrid\"):\n",
    "            textgrid_path = os.path.join(root, file)\n",
    "            alignments = parse_textgrid(textgrid_path)\n",
    "            words_counts, phones_counts  = count_words_phones(alignments, words_counts, phones_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_name = 'words_counts.json'\n",
    "\n",
    "with open(file_name, 'w') as file:\n",
    "    json.dump(words_counts, file, indent=4)\n",
    "\n",
    "file_name1 = 'phones_counts.json'\n",
    "\n",
    "with open(file_name1, 'w') as file:\n",
    "    json.dump(phones_counts, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sil': 5082,\n",
       " 'AY1': 6080,\n",
       " 'W': 8340,\n",
       " 'UH1': 1774,\n",
       " 'D': 17977,\n",
       " 'AH0': 29414,\n",
       " 'N': 26333,\n",
       " 'T': 25466,\n",
       " 'G': 3260,\n",
       " 'IH1': 9551,\n",
       " 'V': 7719,\n",
       " 'HH': 8071,\n",
       " 'AE1': 9801,\n",
       " 'F': 6836,\n",
       " 'P': 6936,\n",
       " 'EH2': 498,\n",
       " 'IY0': 7108,\n",
       " 'R': 15513,\n",
       " 'ER0': 8352,\n",
       " 'DH': 11685,\n",
       " 'AH1': 8134,\n",
       " 'OW1': 4051,\n",
       " 'L': 14913,\n",
       " 'AA1': 4739,\n",
       " 'sp': 16887,\n",
       " 'M': 10784,\n",
       " 'AY2': 351,\n",
       " 'K': 9849,\n",
       " 'EH1': 10214,\n",
       " 'IH0': 13416,\n",
       " 'S': 17463,\n",
       " 'IY1': 6738,\n",
       " 'EY1': 5407,\n",
       " 'Z': 10575,\n",
       " 'UW1': 4305,\n",
       " 'AA2': 207,\n",
       " 'B': 6440,\n",
       " 'OY1': 366,\n",
       " 'SH': 2912,\n",
       " 'ER1': 2142,\n",
       " 'AO1': 4942,\n",
       " 'CH': 2158,\n",
       " 'TH': 2417,\n",
       " 'NG': 3880,\n",
       " 'JH': 1672,\n",
       " 'Y': 2554,\n",
       " 'UW0': 382,\n",
       " 'AW1': 2131,\n",
       " 'AH2': 172,\n",
       " 'OW2': 198,\n",
       " 'IH2': 403,\n",
       " 'spn': 356,\n",
       " 'OW0': 510,\n",
       " 'EH0': 253,\n",
       " 'EY2': 320,\n",
       " 'AE0': 116,\n",
       " 'ZH': 186,\n",
       " 'UW2': 134,\n",
       " 'AY0': 109,\n",
       " 'AE2': 265,\n",
       " 'AO2': 168,\n",
       " 'AA0': 131,\n",
       " 'UH2': 40,\n",
       " 'EY0': 46,\n",
       " 'AW2': 79,\n",
       " 'AO0': 87,\n",
       " 'IY2': 173,\n",
       " 'ER2': 32,\n",
       " 'OY2': 8,\n",
       " 'UH0': 12,\n",
       " 'AW0': 9}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phones_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analize unique/differents tokens for Glove, Bert, Wav2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2703/2703 [07:57<00:00,  5.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os \n",
    "import json\n",
    "\n",
    "directory = '../experiments'\n",
    "model1 = 'wav2vec2'\n",
    "model2 = 'glove'\n",
    "model3 = 'bert-base-uncased'\n",
    "\n",
    "bert_keys = set()\n",
    "glove_keys = set()\n",
    "wav2vec2_keys = set()\n",
    "\n",
    "files = sorted(os.listdir(os.path.join(directory, model1)))\n",
    "for filename in tqdm(files):\n",
    "    identifier = filename.split('_')[-1]\n",
    "    model2_path = f'{model2}/embeddings_words_{identifier}'\n",
    "    model3_path = f'{model3}/embeddings_words_{identifier}'\n",
    "\n",
    "    model1_path = os.path.join(directory, os.path.join(model1,filename))\n",
    "    model2_path = os.path.join(directory, model2_path)\n",
    "    model3_path = os.path.join(directory, model3_path)\n",
    "\n",
    "    if os.path.isfile(model2_path):\n",
    "        with open(model1_path, 'r') as model1_file, open(model2_path, 'r') as model2_file, open(model3_path, 'r') as model3_file:\n",
    "            model1_data = json.load(model1_file)\n",
    "            model2_data = json.load(model2_file)\n",
    "            model3_data = json.load(model3_file)\n",
    "\n",
    "            # Filter keys that appear in all files\n",
    "            audio, _ = os.path.splitext(identifier)\n",
    "            bert_keys.update(model3_data[audio].keys())\n",
    "            glove_keys.update(model2_data[audio].keys())\n",
    "            wav2vec2_keys.update(model1_data[audio].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_keys = bert_keys & glove_keys & wav2vec2_keys\n",
    "len(common_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8372"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_keys = bert_keys | glove_keys | wav2vec2_keys\n",
    "len(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'\",\n",
       " '<unk>',\n",
       " 'ain',\n",
       " \"ain't\",\n",
       " \"alexander's\",\n",
       " 'ambrosch',\n",
       " \"ann's\",\n",
       " \"antonia's\",\n",
       " \"apostle's\",\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " \"aunt's\",\n",
       " \"author's\",\n",
       " 'avrigny',\n",
       " \"baby's\",\n",
       " 'balvastro',\n",
       " 'bambeday',\n",
       " 'beenie',\n",
       " 'bennydeck',\n",
       " 'bergez',\n",
       " 'bhunda',\n",
       " 'birdikins',\n",
       " 'blanco',\n",
       " \"blanco's\",\n",
       " 'blemmyes',\n",
       " 'boolooroo',\n",
       " \"boy's\",\n",
       " 'bozzle',\n",
       " 'brandd',\n",
       " 'brau',\n",
       " 'breadhouse',\n",
       " 'brewer',\n",
       " \"brewer's\",\n",
       " \"bright's\",\n",
       " \"brother's\",\n",
       " \"brown's\",\n",
       " 'burgoynes',\n",
       " \"can't\",\n",
       " 'canyou',\n",
       " \"cap'n\",\n",
       " \"captain's\",\n",
       " \"captive's\",\n",
       " \"catherine's\",\n",
       " 'chaba',\n",
       " 'charlie',\n",
       " \"charlie's\",\n",
       " \"child's\",\n",
       " \"chunky's\",\n",
       " \"church's\",\n",
       " 'collander',\n",
       " \"commandant's\",\n",
       " 'congal',\n",
       " \"connell's\",\n",
       " \"cook's\",\n",
       " 'corncakes',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'creeters',\n",
       " 'culprit',\n",
       " \"culprit's\",\n",
       " 'daguerreotypist',\n",
       " \"dante's\",\n",
       " 'daren',\n",
       " \"daren't\",\n",
       " 'darfhulva',\n",
       " 'darwin',\n",
       " \"darwin's\",\n",
       " \"david's\",\n",
       " \"day's\",\n",
       " 'delaunay',\n",
       " 'delectasti',\n",
       " 'delia',\n",
       " \"delia's\",\n",
       " 'dent',\n",
       " \"dent's\",\n",
       " 'derivatively',\n",
       " \"detective's\",\n",
       " 'dhourra',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'docetes',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doma',\n",
       " \"don't\",\n",
       " 'dorriforth',\n",
       " 'dowle',\n",
       " 'dummy',\n",
       " \"dummy's\",\n",
       " \"e'er\",\n",
       " 'egoisms',\n",
       " \"else's\",\n",
       " \"emperor's\",\n",
       " \"enemy's\",\n",
       " 'epicurism',\n",
       " 'er',\n",
       " \"ever'body\",\n",
       " 'factor',\n",
       " \"factor's\",\n",
       " 'farrell',\n",
       " \"father's\",\n",
       " 'fibi',\n",
       " 'finnacta',\n",
       " 'fjordungr',\n",
       " 'flamelets',\n",
       " 'foster',\n",
       " \"foster's\",\n",
       " 'francisco',\n",
       " \"francisco's\",\n",
       " \"frederick's\",\n",
       " \"friend's\",\n",
       " 'frierson',\n",
       " 'ganny',\n",
       " 'gardar',\n",
       " \"gardener's\",\n",
       " 'gingle',\n",
       " \"girl's\",\n",
       " \"gladden's\",\n",
       " 'gossoons',\n",
       " 'griffin',\n",
       " \"griffin's\",\n",
       " 'groom',\n",
       " \"groom's\",\n",
       " 'guidest',\n",
       " 'guy',\n",
       " \"guy's\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hamilton',\n",
       " \"hamilton's\",\n",
       " \"hand's\",\n",
       " 'hardwigg',\n",
       " \"harry's\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " \"he's\",\n",
       " \"heart's\",\n",
       " \"heaven's\",\n",
       " 'hennerberg',\n",
       " 'hepsey',\n",
       " \"hepzibah's\",\n",
       " \"her's\",\n",
       " 'herbivore',\n",
       " \"here's\",\n",
       " 'heuchera',\n",
       " \"hilda's\",\n",
       " 'hochheimer',\n",
       " 'homoiousios',\n",
       " 'homoousios',\n",
       " \"hour's\",\n",
       " 'hugh',\n",
       " \"hugh's\",\n",
       " 'hurstwood',\n",
       " \"husband's\",\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'impara',\n",
       " 'inexhausted',\n",
       " 'irolg',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " \"it's\",\n",
       " 'jack',\n",
       " \"jack's\",\n",
       " \"jem's\",\n",
       " \"jim's\",\n",
       " \"journey's\",\n",
       " \"julian's\",\n",
       " 'kate',\n",
       " \"kate's\",\n",
       " 'khosala',\n",
       " \"king's\",\n",
       " \"kitty's\",\n",
       " 'lacquey',\n",
       " \"lady's\",\n",
       " 'lassen',\n",
       " \"lassen's\",\n",
       " \"latter's\",\n",
       " \"laugh'd\",\n",
       " 'leighton',\n",
       " \"leighton's\",\n",
       " \"let's\",\n",
       " \"letty's\",\n",
       " 'libano',\n",
       " 'linnell',\n",
       " \"linnell's\",\n",
       " 'll',\n",
       " \"look'd\",\n",
       " 'lucy',\n",
       " \"lucy's\",\n",
       " 'macklewain',\n",
       " 'magazzino',\n",
       " 'mainhall',\n",
       " \"man's\",\n",
       " \"mary's\",\n",
       " 'mason',\n",
       " \"mason's\",\n",
       " \"men's\",\n",
       " \"merchant's\",\n",
       " \"minister's\",\n",
       " 'minnetaki',\n",
       " \"minnie's\",\n",
       " 'moling',\n",
       " \"moment's\",\n",
       " \"mother's\",\n",
       " \"mule's\",\n",
       " 'mulrady',\n",
       " 'myrdals',\n",
       " 'n',\n",
       " \"nature's\",\n",
       " \"need'st\",\n",
       " \"newsome's\",\n",
       " 'nicless',\n",
       " \"nobody's\",\n",
       " 'noirtier',\n",
       " \"norman's\",\n",
       " \"north's\",\n",
       " 'novatians',\n",
       " \"o'clock\",\n",
       " \"o'farrell\",\n",
       " 'officinale',\n",
       " 'olbinett',\n",
       " \"one's\",\n",
       " 'opponent',\n",
       " \"opponent's\",\n",
       " 'ossipon',\n",
       " \"other's\",\n",
       " 'oughtn',\n",
       " \"oughtn't\",\n",
       " 'overmasters',\n",
       " \"papa's\",\n",
       " 'parrishes',\n",
       " \"paul's\",\n",
       " \"people's\",\n",
       " \"peter's\",\n",
       " 'petronels',\n",
       " 'philly',\n",
       " \"philly's\",\n",
       " \"philosopher's\",\n",
       " 'pigstye',\n",
       " 'pinkies',\n",
       " 'plebeianism',\n",
       " \"possess'd\",\n",
       " 'pravity',\n",
       " 'presty',\n",
       " \"professor's\",\n",
       " 'quakerish',\n",
       " 'quinci',\n",
       " 'raimented',\n",
       " 'rainbow',\n",
       " \"rainbow's\",\n",
       " \"randal's\",\n",
       " 'rangitata',\n",
       " 'rathskellers',\n",
       " 'razetta',\n",
       " 'rector',\n",
       " \"rector's\",\n",
       " 'recuperations',\n",
       " 'regent',\n",
       " \"regent's\",\n",
       " 'resign',\n",
       " \"resign'd\",\n",
       " 'riverlike',\n",
       " \"sailor's\",\n",
       " 'saknussemm',\n",
       " 'sandyseal',\n",
       " 'satisfier',\n",
       " \"saturday's\",\n",
       " 'scheiler',\n",
       " \"second's\",\n",
       " \"shakespeare's\",\n",
       " 'shampooer',\n",
       " \"she'd\",\n",
       " \"she's\",\n",
       " \"sheep's\",\n",
       " 'shimerda',\n",
       " 'shimerdas',\n",
       " \"ship's\",\n",
       " 'shoplets',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'sippet',\n",
       " 'sippets',\n",
       " \"sister's\",\n",
       " 'skint',\n",
       " 'sneffels',\n",
       " \"somebody's\",\n",
       " 'spilth',\n",
       " 'sprucewood',\n",
       " 'st',\n",
       " 'stepp',\n",
       " \"stepp'd\",\n",
       " \"stevie's\",\n",
       " 'stommick',\n",
       " \"stone's\",\n",
       " 'stupirti',\n",
       " 'sudvestr',\n",
       " \"sun's\",\n",
       " \"swift's\",\n",
       " \"sydney's\",\n",
       " 'synesius',\n",
       " \"tad's\",\n",
       " 'tarrinzeau',\n",
       " 'telemetering',\n",
       " 'testbridge',\n",
       " \"that'll\",\n",
       " \"that's\",\n",
       " 'theosophies',\n",
       " \"there'd\",\n",
       " \"there's\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'tishimingo',\n",
       " \"tom's\",\n",
       " \"tony's\",\n",
       " 'trampe',\n",
       " \"trevelyan's\",\n",
       " 'troke',\n",
       " \"trot's\",\n",
       " 'tumble',\n",
       " 'unbarring',\n",
       " 'uncloud',\n",
       " 'unsoldierly',\n",
       " 'untruss',\n",
       " 'untrussing',\n",
       " 've',\n",
       " 'vendhya',\n",
       " 'veolan',\n",
       " 'verloc',\n",
       " 'vinos',\n",
       " \"wabi's\",\n",
       " 'wabigoon',\n",
       " \"wabigoon's\",\n",
       " 'wahiti',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " \"waverley's\",\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we've\",\n",
       " 'weiser',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'westmere',\n",
       " \"what's\",\n",
       " \"who's\",\n",
       " \"wife's\",\n",
       " 'wildering',\n",
       " \"witch's\",\n",
       " \"woman's\",\n",
       " \"won't\",\n",
       " 'woonga',\n",
       " 'woongas',\n",
       " \"world's\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " \"wrangler's\",\n",
       " \"year's\",\n",
       " 'yokul',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'yulka',\n",
       " 'yundt',\n",
       " 'zingiber'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_keys - common_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = 'common_keys.json'\n",
    "\n",
    "with open(common, 'w') as file:\n",
    "    json.dump(list(common_keys), file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2703 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2703/2703 [14:02<00:00,  3.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "model1 = 'wav2vec2'\n",
    "directory = '../experiments'\n",
    "words_order = []\n",
    "files = sorted(os.listdir(os.path.join(directory, model1)))\n",
    "model2 = 'bert-base-uncased'\n",
    "model3 = 'glove'\n",
    "\n",
    "for filename in tqdm(files):\n",
    "    identifier = filename.split('_')[-1]\n",
    "    model2_path = f'{model2}/embeddings_words_{identifier}'\n",
    "    model3_path = f'{model3}/embeddings_words_{identifier}'\n",
    "\n",
    "    model1_path = os.path.join(directory, f'{model1}/{filename}')\n",
    "    model2_path = os.path.join(directory, model2_path)\n",
    "    model3_path = os.path.join(directory, model3_path)\n",
    "\n",
    "    if os.path.isfile(model2_path):\n",
    "        with open(model1_path, 'r') as model1_file, open(model2_path, 'r') as model2_file, open(model3_path, 'r') as model3_file:\n",
    "            model1_data = json.load(model1_file)\n",
    "            model2_data = json.load(model2_file)\n",
    "            model3_data = json.load(model3_file)\n",
    "\n",
    "            # Filter keys that appear in all files\n",
    "            audio, _ = os.path.splitext(identifier)\n",
    "            common_keys = set(model1_data[audio].keys()) & set(model2_data[audio].keys()) & set(model3_data[audio].keys())\n",
    "            common_keys = sorted(common_keys)\n",
    "            for key in common_keys:\n",
    "                words_order.append([key, identifier]) \n",
    "        \n",
    "with open(os.path.join('words_in_order_audio.json'), 'w') as f:\n",
    "    json.dump(words_order, f) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
